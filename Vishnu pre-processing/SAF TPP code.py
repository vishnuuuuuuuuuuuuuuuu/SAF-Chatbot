# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V2iR7RReLLw-sFLrQ-AIcUNC12eHW7ld

# Text Pre-Processing Pipeline

Installing relevant packages/libraries
"""

!pip install pytesseract transformers
!pip install pymupdf camelot-py[cv] pdf2image

import os
import fitz  # PyMuPDF
import camelot
import json
import re

"""Text clean up and pre-processing"""

input_pdf_dir = '/content/raw_pdfs'
output_json_dir = '/content/final_json_clean'
os.makedirs(output_json_dir, exist_ok=True)

SAF_KEYWORDS = [
    'saf', 'sustainable aviation fuel', 'synthetic aviation fuel', 'renewable aviation fuel',
    'esaf', 'efuels', 'fuel', 'fuel burn', 'fuel uplift', 'carbon dioxide',
    'emissions', 'co2', 'carbon offset', 'scope 1', 'scope 2', 'scope 3',
    'greenhouse gas', 'ghg', 'aviation fuel', 'esg report', 'sustainability report',
    'sustainability', 'environmental report', 'environmental strategy', 'environmental',
    'euets', 'european union emissions trading scheme',
    'chief sustainability officer', 'vp sustainability', 'head of sustainability',
    'net zero', 'carbon reduction', 'green'
]

def clean_text(text):
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = text.replace('CO₂', 'CO2')
    return text.strip()

def contains_keyword(text, keywords):
    text = text.lower()
    return any(re.search(rf"\b{re.escape(kw)}", text) for kw in keywords)

def extract_tables(pdf_path):
    tables_by_page = {}
    try:
        tables = camelot.read_pdf(pdf_path, pages='all', flavor='stream')
        for t in tables:
            page = int(t.page)
            table_json = t.df.to_dict()
            tables_by_page.setdefault(page, []).append(table_json)
    except Exception as e:
        print(f"⚠️ Table extraction error in {pdf_path}: {e}")
    return tables_by_page

def extract_text_and_tables(pdf_path, keywords=SAF_KEYWORDS, min_pages=5):
    doc = fitz.open(pdf_path)
    tables_by_page = extract_tables(pdf_path)
    result_pages = []

    if len(doc) <= min_pages:
        page_indices = range(len(doc))
    else:
        page_indices = [i for i in range(len(doc)) if contains_keyword(doc[i].get_text(), keywords)]

    for i in page_indices:
        page_num = i + 1
        page_text = clean_text(doc[i].get_text())
        page_tables = tables_by_page.get(page_num, [])

        result_pages.append({
            "page": page_num,
            "text": page_text if page_text else None,
            "tables": page_tables if page_tables else []
        })

    return result_pages

def process_pdf(pdf_path, output_dir):
    filename = os.path.basename(pdf_path).replace('.pdf', '')
    pages_data = extract_text_and_tables(pdf_path)

    output = {
        "file": filename,
        "num_pages": len(pages_data),
        "pages": pages_data
    }

    output_path = os.path.join(output_dir, f"{filename}.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2)
    print(f"✅ Saved: {output_path}")

def process_all_pdfs(input_dir, output_dir):
    for fname in os.listdir(input_dir):
        if fname.lower().endswith('.pdf'):
            process_pdf(os.path.join(input_dir, fname), output_dir)

process_all_pdfs(input_pdf_dir, output_json_dir)

"""Chunking"""

def split_text_into_chunks(text, max_chars=1500):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    chunks = []
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_chars:
            current_chunk += " " + sentence
        else:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = sentence
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    return chunks


def chunk_json_file(input_path, output_path, start_chunk_id=1, max_chars=1500):
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    source_file = data.get("file", os.path.basename(input_path).replace(".json", ""))
    page_data = data.get("pages", [])
    all_chunks = []
    chunk_id = start_chunk_id

    for page in page_data:
        page_num = page.get("page", None)
        text = page.get("text", "")
        tables = page.get("tables", [])

        # Text chunks
        text_chunks = split_text_into_chunks(text, max_chars=max_chars)
        for chunk in text_chunks:
            all_chunks.append({
                "chunk_id": f"{chunk_id:08d}",
                "chunk_type": "text",
                "source_file": source_file,
                "page_num": page_num,
                "text": chunk
            })
            chunk_id += 1

        # Table chunks
        for table in tables:
            all_chunks.append({
                "chunk_id": f"{chunk_id:08d}",
                "chunk_type": "table",
                "source_file": source_file,
                "page_num": page_num,
                "table": table
            })
            chunk_id += 1

    # Save output
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump({"chunks": all_chunks}, f, indent=2)

    print(f"✅ {os.path.basename(output_path)} saved with chunks {start_chunk_id:08d}–{chunk_id - 1:08d}")
    return chunk_id  # return next starting ID


def chunk_all_files(input_folder, output_folder, max_chars=1500):
    os.makedirs(output_folder, exist_ok=True)
    chunk_id_counter = 1206

    for fname in sorted(os.listdir(input_folder)):
        if fname.endswith(".json"):
            input_path = os.path.join(input_folder, fname)
            output_path = os.path.join(output_folder, fname.replace(".json", "_chunked.json"))
            chunk_id_counter = chunk_json_file(
                input_path, output_path,
                start_chunk_id=chunk_id_counter,
                max_chars=max_chars
            )

# Example usage:
chunk_all_files(
    input_folder="/content/final_json_clean",
    output_folder="/content/chunked_jsons",
    max_chars=1500
)