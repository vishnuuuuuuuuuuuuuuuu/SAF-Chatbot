{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xbLGXUZtC8F6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69c97ae6b7524098b68349ae6aa34279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d1c18c7214b40ea84b995da5acf47da",
              "IPY_MODEL_f53b2c1c1c5e45cbad184dfafd5930a6",
              "IPY_MODEL_e98ffca62c59413ea975bbaf92dbc699"
            ],
            "layout": "IPY_MODEL_fa6179e95784406b878302ed2533bb6e"
          }
        },
        "1d1c18c7214b40ea84b995da5acf47da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceede3c16a9644a58978c5b560e57bda",
            "placeholder": "​",
            "style": "IPY_MODEL_39b3e77d056349a49c62cdf1008c110e",
            "value": "Evaluating: 100%"
          }
        },
        "f53b2c1c1c5e45cbad184dfafd5930a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00630523779042f091c75a6919a6b1cd",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae5b4328366842029749c2316e3e2e2e",
            "value": 300
          }
        },
        "e98ffca62c59413ea975bbaf92dbc699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d60dae8c77ec4bae85a40945c409a783",
            "placeholder": "​",
            "style": "IPY_MODEL_e48ba043a16142b5bfe18e0b7f42f5b7",
            "value": " 300/300 [03:22&lt;00:00,  2.76s/it]"
          }
        },
        "fa6179e95784406b878302ed2533bb6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceede3c16a9644a58978c5b560e57bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b3e77d056349a49c62cdf1008c110e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00630523779042f091c75a6919a6b1cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae5b4328366842029749c2316e3e2e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d60dae8c77ec4bae85a40945c409a783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48ba043a16142b5bfe18e0b7f42f5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "KhenjCWJT06o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain faiss-cpu langchain_openai ragas langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UyMvVqyriSEw",
        "outputId": "e2e1e352-82a8-43bf-9a34-57dfc3fa5f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ragas\n",
            "  Downloading ragas-0.2.15-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.86.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from ragas) (2.14.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ragas) (1.6.0)\n",
            "Collecting appdirs (from ragas)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting diskcache>=5.6.3 (from ragas)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets->ragas) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.33.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->ragas) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets->ragas) (1.1.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.24-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ragas-0.2.15-py3-none-any.whl (190 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: appdirs, python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, diskcache, typing-inspect, pydantic-settings, dataclasses-json, langchain_openai, langchain-community, ragas\n",
            "Successfully installed appdirs-1.4.4 dataclasses-json-0.6.7 diskcache-5.6.3 faiss-cpu-1.11.0 httpx-sse-0.4.0 langchain-community-0.3.25 langchain_openai-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 ragas-0.2.15 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain faiss-cpu"
      ],
      "metadata": {
        "id": "ujrWtOqntHXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2963afc-8719-42e5-c9bd-2201a3871cd1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API"
      ],
      "metadata": {
        "id": "X-AML8NWdXYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_ENDPOINT = \"https://ai-research-proxy.azurewebsites.net\"\n",
        "API_KEY = \"AAA\""
      ],
      "metadata": {
        "id": "AUiw-rddR98L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding (only use if no FAISS database yet)"
      ],
      "metadata": {
        "id": "2yGyUpekR-yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "ZnmqP8bVUfPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding model\n",
        "embedding = OpenAIEmbeddings(\n",
        "    openai_api_key=API_KEY,\n",
        "    model=\"text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "def get_directory_size(path):\n",
        "    total = 0\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            total += os.path.getsize(os.path.join(dirpath, f))\n",
        "    return total\n",
        "\n",
        "def embed_in_batches(docs, embedding, batch_size=100):\n",
        "    \"\"\"\n",
        "    Embed documents in batches and return a combined FAISS store.\n",
        "    \"\"\"\n",
        "    vectorstore = None\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i+batch_size]\n",
        "        print(f\"  → Embedding batch {i} to {i+len(batch)}...\")\n",
        "        batch_store = FAISS.from_documents(batch, embedding)\n",
        "\n",
        "        if vectorstore is None:\n",
        "            vectorstore = batch_store\n",
        "        else:\n",
        "            vectorstore.merge_from(batch_store)\n",
        "    return vectorstore\n",
        "\n",
        "# Upload and process file\n",
        "print(\"Upload your JSON files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "vectorstore = None\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"\\nProcessing: {filename}\")\n",
        "\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to parse {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    docs = []\n",
        "\n",
        "    for item in data.get(\"chunks\", []):\n",
        "        chunk_id = item.get(\"chunk_id\")\n",
        "        chunk_type = item.get(\"chunk_type\", \"\")\n",
        "        source_file = item.get(\"source_file\", filename)\n",
        "        page_num = item.get(\"page_num\", None)\n",
        "\n",
        "        metadata = {\n",
        "            \"chunk_id\": chunk_id,\n",
        "            \"source_file\": source_file,\n",
        "            \"page_num\": page_num\n",
        "        }\n",
        "\n",
        "        if chunk_type == \"text\":\n",
        "            text = item.get(\"text\", \"\")\n",
        "            if text.strip():\n",
        "                docs.append(Document(\n",
        "                    page_content=text,\n",
        "                    metadata=metadata\n",
        "                ))\n",
        "\n",
        "        elif chunk_type == \"table\":\n",
        "            table = item.get(\"table\", {})\n",
        "            table_str = \"\\n\".join([\n",
        "                \" | \".join(row.values())\n",
        "                for row in table.values()\n",
        "            ])\n",
        "            docs.append(Document(\n",
        "                page_content=table_str,\n",
        "                metadata=metadata\n",
        "            ))\n",
        "\n",
        "    if not docs:\n",
        "        print(f\"No documents extracted from {filename}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Embedding {len(docs)} segments in batches...\")\n",
        "    temp_store = embed_in_batches(docs, embedding, batch_size=100)\n",
        "\n",
        "    if vectorstore is None:\n",
        "        vectorstore = temp_store\n",
        "    else:\n",
        "        vectorstore.merge_from(temp_store)\n",
        "\n",
        "    vectorstore.save_local(\"faiss_index\")\n",
        "    print(f\"Stored {filename} → FAISS index updated.\")\n",
        "\n",
        "# Zip and download\n",
        "size_mb = get_directory_size(\"faiss_index\") / (1024 * 1024)\n",
        "print(f\"\\nFinal FAISS index size: {size_mb:.2f} MB\")\n",
        "\n",
        "shutil.make_archive(\"faiss_index\", 'zip', \"faiss_index\")\n",
        "os.rename(\"faiss_index.zip\", \"faiss_27reports.zip\")\n",
        "files.download(\"faiss_27reports.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "33W-tmDhSos5",
        "outputId": "16e8ce23-15a2-4d06-c122-38d55a332652",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your JSON files:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e9bd0d3-e90d-47f0-bd92-50d35f1eec54\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e9bd0d3-e90d-47f0-bd92-50d35f1eec54\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Aer Lingus Sustainability_chunked.json to Aer Lingus Sustainability_chunked.json\n",
            "Saving Air Iberia Sustainability_chunked.json to Air Iberia Sustainability_chunked.json\n",
            "Saving Air NZ GHG_chunked.json to Air NZ GHG_chunked.json\n",
            "Saving Air NZ Sustainability_chunked.json to Air NZ Sustainability_chunked.json\n",
            "Saving Alaska airlines Sustainability_chunked.json to Alaska airlines Sustainability_chunked.json\n",
            "Saving American Airlines Sustainaibility_chunked.json to American Airlines Sustainaibility_chunked.json\n",
            "Saving ANA annual report _chunked.json to ANA annual report _chunked.json\n",
            "Saving Cathay Pacific Sustainability_chunked.json to Cathay Pacific Sustainability_chunked.json\n",
            "Saving Delta Sustainability_chunked.json to Delta Sustainability_chunked.json\n",
            "Saving DHL Sustainability_chunked.json to DHL Sustainability_chunked.json\n",
            "Saving Emirates Report_chunked.json to Emirates Report_chunked.json\n",
            "Saving FedEX sustainability_chunked.json to FedEX sustainability_chunked.json\n",
            "Saving finnair annual report_chunked.json to finnair annual report_chunked.json\n",
            "Saving IAG Sustainability Report (BA)_chunked.json to IAG Sustainability Report (BA)_chunked.json\n",
            "Saving Japan Airlines Annual Report_chunked.json to Japan Airlines Annual Report_chunked.json\n",
            "Saving KLM Sustainability_chunked.json to KLM Sustainability_chunked.json\n",
            "Saving Latam Airlines Annual_chunked.json to Latam Airlines Annual_chunked.json\n",
            "Saving Lufthansa Sustainability_chunked.json to Lufthansa Sustainability_chunked.json\n",
            "Saving Malaysia Airlines Sustainability Airlines_chunked.json to Malaysia Airlines Sustainability Airlines_chunked.json\n",
            "Saving Qantas Sustainability_chunked.json to Qantas Sustainability_chunked.json\n",
            "Saving Ryan Air Sustainability report_chunked.json to Ryan Air Sustainability report_chunked.json\n",
            "Saving Southwest Airlines Sustainbility_chunked.json to Southwest Airlines Sustainbility_chunked.json\n",
            "Saving Srilankan Airlines Annual_chunked.json to Srilankan Airlines Annual_chunked.json\n",
            "Saving united Sustainability_chunked.json to united Sustainability_chunked.json\n",
            "Saving UPS Sustainability_chunked.json to UPS Sustainability_chunked.json\n",
            "Saving Virgin Atlantic Annual Report_chunked.json to Virgin Atlantic Annual Report_chunked.json\n",
            "Saving Vueling Sustaibility_chunked.json to Vueling Sustaibility_chunked.json\n",
            "\n",
            "Processing: Aer Lingus Sustainability_chunked.json\n",
            "Embedding 100 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "Stored Aer Lingus Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Air Iberia Sustainability_chunked.json\n",
            "Embedding 539 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 400...\n",
            "  → Embedding batch 400 to 500...\n",
            "  → Embedding batch 500 to 539...\n",
            "Stored Air Iberia Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Air NZ GHG_chunked.json\n",
            "Embedding 62 segments in batches...\n",
            "  → Embedding batch 0 to 62...\n",
            "Stored Air NZ GHG_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Air NZ Sustainability_chunked.json\n",
            "Embedding 191 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 191...\n",
            "Stored Air NZ Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Alaska airlines Sustainability_chunked.json\n",
            "Embedding 247 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 247...\n",
            "Stored Alaska airlines Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: American Airlines Sustainaibility_chunked.json\n",
            "Embedding 356 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 356...\n",
            "Stored American Airlines Sustainaibility_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: ANA annual report _chunked.json\n",
            "Embedding 272 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 272...\n",
            "Stored ANA annual report _chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Cathay Pacific Sustainability_chunked.json\n",
            "Embedding 598 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 400...\n",
            "  → Embedding batch 400 to 500...\n",
            "  → Embedding batch 500 to 598...\n",
            "Stored Cathay Pacific Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Delta Sustainability_chunked.json\n",
            "Embedding 203 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 203...\n",
            "Stored Delta Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: DHL Sustainability_chunked.json\n",
            "Embedding 148 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 148...\n",
            "Stored DHL Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Emirates Report_chunked.json\n",
            "Embedding 364 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 364...\n",
            "Stored Emirates Report_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: FedEX sustainability_chunked.json\n",
            "Embedding 164 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 164...\n",
            "Stored FedEX sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: finnair annual report_chunked.json\n",
            "Embedding 847 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 400...\n",
            "  → Embedding batch 400 to 500...\n",
            "  → Embedding batch 500 to 600...\n",
            "  → Embedding batch 600 to 700...\n",
            "  → Embedding batch 700 to 800...\n",
            "  → Embedding batch 800 to 847...\n",
            "Stored finnair annual report_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: IAG Sustainability Report (BA)_chunked.json\n",
            "Embedding 397 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 397...\n",
            "Stored IAG Sustainability Report (BA)_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Japan Airlines Annual Report_chunked.json\n",
            "Embedding 665 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 400...\n",
            "  → Embedding batch 400 to 500...\n",
            "  → Embedding batch 500 to 600...\n",
            "  → Embedding batch 600 to 665...\n",
            "Stored Japan Airlines Annual Report_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: KLM Sustainability_chunked.json\n",
            "Embedding 263 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 263...\n",
            "Stored KLM Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Latam Airlines Annual_chunked.json\n",
            "Embedding 749 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 400...\n",
            "  → Embedding batch 400 to 500...\n",
            "  → Embedding batch 500 to 600...\n",
            "  → Embedding batch 600 to 700...\n",
            "  → Embedding batch 700 to 749...\n",
            "Stored Latam Airlines Annual_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Lufthansa Sustainability_chunked.json\n",
            "Embedding 108 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 108...\n",
            "Stored Lufthansa Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Malaysia Airlines Sustainability Airlines_chunked.json\n",
            "Embedding 276 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 276...\n",
            "Stored Malaysia Airlines Sustainability Airlines_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Qantas Sustainability_chunked.json\n",
            "Embedding 374 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 374...\n",
            "Stored Qantas Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Ryan Air Sustainability report_chunked.json\n",
            "Embedding 230 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 230...\n",
            "Stored Ryan Air Sustainability report_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Southwest Airlines Sustainbility_chunked.json\n",
            "Embedding 336 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 336...\n",
            "Stored Southwest Airlines Sustainbility_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Srilankan Airlines Annual_chunked.json\n",
            "Embedding 138 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 138...\n",
            "Stored Srilankan Airlines Annual_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: united Sustainability_chunked.json\n",
            "Embedding 8 segments in batches...\n",
            "  → Embedding batch 0 to 8...\n",
            "Stored united Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: UPS Sustainability_chunked.json\n",
            "Embedding 184 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 184...\n",
            "Stored UPS Sustainability_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Virgin Atlantic Annual Report_chunked.json\n",
            "Embedding 350 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 300...\n",
            "  → Embedding batch 300 to 350...\n",
            "Stored Virgin Atlantic Annual Report_chunked.json → FAISS index updated.\n",
            "\n",
            "Processing: Vueling Sustaibility_chunked.json\n",
            "Embedding 207 segments in batches...\n",
            "  → Embedding batch 0 to 100...\n",
            "  → Embedding batch 100 to 200...\n",
            "  → Embedding batch 200 to 207...\n",
            "Stored Vueling Sustaibility_chunked.json → FAISS index updated.\n",
            "\n",
            "Final FAISS index size: 62.84 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5b973a2e-25fb-4cb8-afd4-a42b18ac92f2\", \"faiss_27reports.zip\", 46414799)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access LLM & test"
      ],
      "metadata": {
        "id": "gMACVc-Hirw4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea8ybwd6Ualp",
        "outputId": "0e75665e-e52d-4445-ee59-e9f467f80a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2371223853.py:20: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chat(messages)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Litellm is amazing because it combines advanced language processing capabilities with user-friendly interfaces, making it accessible and powerful for a wide range of applications.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 41, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7a53abb7a2', 'id': 'chatcmpl-Bk7TXsSJOmeecs7slw5DYCg5vfgxb', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--b4c34e3b-47d6-4c5f-b398-fa104d49bf44-0' usage_metadata={'input_tokens': 41, 'output_tokens': 30, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_base= API_ENDPOINT,\n",
        "    openai_api_key= API_KEY,\n",
        "    model = \"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\n",
        "        content=\"You are a helpful assistant that im using to make a test request to.\"\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"test from litellm. tell me why it's amazing in 1 sentence\"\n",
        "    ),\n",
        "]\n",
        "response = chat(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From https://ai-research-proxy.azurewebsites.net/ui/?userID=e8fc58ee-ae1a-4026-9a80-8fa56df2b6bd&page=api_ref"
      ],
      "metadata": {
        "id": "eI6JuVMKiH1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip"
      ],
      "metadata": {
        "id": "ncJRB8lInJJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload .zip\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "rbEmcYao6wjN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5731a4bd-6522-4122-ff6d-ec5d4744731e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-881da444-d5ec-4ef6-926f-54411983f4c3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-881da444-d5ec-4ef6-926f-54411983f4c3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving faiss_27reports.zip to faiss_27reports.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"faiss_27reports.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"faiss_index\")"
      ],
      "metadata": {
        "id": "YfM1pGfknLye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding = OpenAIEmbeddings(\n",
        "    openai_api_base=API_ENDPOINT,\n",
        "    openai_api_key=API_KEY,\n",
        "    model=\"text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "vectorstore = FAISS.load_local(\"faiss_index\", embedding, allow_dangerous_deserialization=True)\n",
        "print(\"FAISS index loaded!\")\n"
      ],
      "metadata": {
        "id": "XdGymTQknONi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a25470-9475-41a2-e1b2-656357d397aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-295396684.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding = OpenAIEmbeddings(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Retriever"
      ],
      "metadata": {
        "id": "Kv1HByCCmOYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.load import dumps, loads\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import Document\n",
        "from operator import itemgetter\n",
        "import re\n",
        "\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "gS_IEEPumMdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "id": "Cv_-t0E1t0Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question"
      ],
      "metadata": {
        "id": "yrZ0Fz2t0Igv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much SAF did KLM use in 2024?\""
      ],
      "metadata": {
        "id": "peWfyQTv0GEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive"
      ],
      "metadata": {
        "id": "8sGaMpy7Mvzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Straight up takes the question and finds the most relevant parts of the documents to answer it.\n",
        "\n",
        "Usually not as good as advanced techniques, especially for more complex questions."
      ],
      "metadata": {
        "id": "GomrJwd9lpRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_references = \"\\n\\n\".join(\n",
        "    f\"Title: {doc.metadata.get('source_file', 'Unknown Source')}\\nPage: {doc.metadata.get('page_num', 'Unknown Page')}\\nContent: {doc.page_content}\"\n",
        "    for doc in retriever.invoke(question)\n",
        ")\n",
        "\n",
        "\n",
        "naive_template = [\n",
        "    SystemMessage(content=\"\"\" You are a document analyst.\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.\n",
        "You follow these instructions and use only these documents.\n",
        "You reference the sources in a reference list at the end, using the original source file name under 'source_file' and the page number under 'page_num'. In the list, don't repeat the same reference multiple times.\n",
        "You can recognize when no document is a good match and return \"I don't know\". \"\"\"),\n",
        "    HumanMessage(content=\"Here are the documents: \" + doc_references),\n",
        "    HumanMessage(content=\"Here are the instructions: \" + question)\n",
        "]\n",
        "\n",
        "display(Markdown(chat.invoke(naive_template).content))\n"
      ],
      "metadata": {
        "id": "VKXrJuxhMx0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "b9688792-20fa-42da-cc36-bdba796035db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used 57 kilotons of Sustainable Aviation Fuel (SAF). \n\nReferences:\n- KLM Sustainability, page 42."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced"
      ],
      "metadata": {
        "id": "YpB1b-DjIIds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All codes adapted from:\n",
        "https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb"
      ],
      "metadata": {
        "id": "veDJzEgOITXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposition - Subquestions"
      ],
      "metadata": {
        "id": "NloM_ketrL5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It creates subquestion of the question asked by the user.\n",
        "\n",
        "**Recursively**:\n",
        "\n",
        "Answers each sub-question in sequence, carrying forward the context from previous ones.\n",
        "\n",
        "\n",
        "**Individually**:\n",
        "\n",
        "Answers each subquestion individually, then synthesize them in a single answer."
      ],
      "metadata": {
        "id": "g83FOrqfi2hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subquestions_template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question, the question regards the one or more aviation sustainability report.\n",
        "The goal is to break down the input into a set of sub-questions that can be answered in isolation.\n",
        "Generate multiple search queries related to: {question}\n",
        "Output (3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(subquestions_template)"
      ],
      "metadata": {
        "id": "TYHlcKBZrLEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "generate_queries_decomposition = ( prompt_decomposition | chat | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "# Run\n",
        "subquestions = generate_queries_decomposition.invoke({\"question\":question})"
      ],
      "metadata": {
        "id": "NByt0Oxwsju8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subquestions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6xo0XfwuF2n",
        "outputId": "76696413-e58f-4aaf-c645-683b91a266fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. What is the total amount of Sustainable Aviation Fuel (SAF) used by KLM in 2024?',\n",
              " \"2. How does KLM's SAF usage in 2024 compare to previous years?\",\n",
              " \"3. What percentage of KLM's total fuel consumption in 2024 was made up of SAF?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposition - Recursively answered"
      ],
      "metadata": {
        "id": "iG7PR8jq6gAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RECURSIVE\n",
        "recursive_template = \"\"\"You are a document analyst.\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.\n",
        "You follow these instructions and use only these documents.\n",
        "\n",
        "Here is the question you need to answer fully:\n",
        "\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question (each chunk may include source metadata):\n",
        "\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer the question fully.\n",
        "\n",
        "When answering, clearly state the source and page number in parentheses after each key fact or figure.\n",
        "If information is drawn from the context, include a citation in this format: (source: {{source_file}}, page {{page_num}}).\n",
        "\n",
        "Answer with references:\n",
        "Add a reference list at the end, listing all the references used in format: source: {{source_file}}, page {{page_num}}. In the list, don't repeat the same reference multiple times.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "recursive_prompt = ChatPromptTemplate.from_template(recursive_template)"
      ],
      "metadata": {
        "id": "nPEkiNvnuLDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_qa_pair(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in subquestions:\n",
        "\n",
        "    rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever,\n",
        "     \"question\": itemgetter(\"question\"),\n",
        "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
        "    | recursive_prompt\n",
        "    | chat\n",
        "    | StrOutputParser())\n",
        "\n",
        "    recursive_answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
        "    q_a_pair = format_qa_pair(q,recursive_answer)\n",
        "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
        "\n",
        "display(Markdown(recursive_answer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "nr5-8s1NvThs",
        "outputId": "ed54aa53-75dd-4434-922d-82f95b0429b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM's total fuel consumption was 3,273 kilotons of conventional aviation fuel, and it used 57 kilotons of Sustainable Aviation Fuel (SAF) (source: KLM Sustainability, page 42). To calculate the percentage of KLM's total fuel consumption that was made up of SAF, we can use the following formula:\n\n\\[\n\\text{Percentage of SAF} = \\left( \\frac{\\text{SAF used}}{\\text{Total fuel consumption}} \\right) \\times 100\n\\]\n\nSubstituting the values:\n\n\\[\n\\text{Percentage of SAF} = \\left( \\frac{57 \\text{ kilotons}}{3,273 \\text{ kilotons}} \\right) \\times 100 \\approx 1.74\\%\n\\]\n\nThus, approximately **1.74%** of KLM's total fuel consumption in 2024 was made up of Sustainable Aviation Fuel (SAF).\n\n### References\nsource: KLM Sustainability, page 42"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposition - Answered individually"
      ],
      "metadata": {
        "id": "POOpR6hW7aqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INDIVIDUAL\n",
        "individual_template0 = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are a document analyst.\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.\n",
        "You follow these instructions and use only these documents.\n",
        "\n",
        "Answer the question based on the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "When answering, clearly state the source and page number in parentheses after each key fact or figure.\n",
        "\n",
        "If information is drawn from the context, include a citation in this format: (source: {{source_file}}, page {{page_num}}).\n",
        "\n",
        "Answer with references:\n",
        "\n",
        "\n",
        "Add a reference list at the end, listing all the references used in format: source: {{source_file}}, page {{page_num}}. In the list, don't repeat the same reference multiple times.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "def individual(question, individual_template, sub_question_generator_chain):\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\": question})\n",
        "\n",
        "    individual_results = []\n",
        "    all_docs = []\n",
        "    for sub_question in sub_questions:\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "        all_docs.extend(retrieved_docs)\n",
        "\n",
        "\n",
        "        context_text = \"\"\n",
        "        for doc in retrieved_docs:\n",
        "            source_info = f\"(Source: {doc.metadata.get('source_file', 'Unknown')}, Page: {doc.metadata.get('page_num', 'N/A')})\"\n",
        "            context_text += f\"{doc.page_content}\\n{source_info}\\n\\n\"\n",
        "\n",
        "        answer_individual0 = (individual_template | chat | StrOutputParser()).invoke({\n",
        "            \"context\": context_text,\n",
        "            \"question\": sub_question\n",
        "        })\n",
        "        individual_results.append(answer_individual0)\n",
        "\n",
        "    return individual_results, sub_questions, all_docs ##\n",
        "\n",
        "answers_i, subquestions_i, all_docs_i = individual(question, individual_template0, generate_queries_decomposition)"
      ],
      "metadata": {
        "id": "a__SKuHDs5R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef89c8c3-443b-4ce2-e938-668c2998663b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-17-1419450673.py:33: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_qa_pairs(subquestions, answers):\n",
        "    \"\"\"Format Q and A pairs\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    for i, (question, answer) in enumerate(zip(subquestions, answers), start=1):\n",
        "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "context = format_qa_pairs(subquestions_i, answers_i)\n",
        "\n",
        "# Prompt\n",
        "individual_template1 = \"\"\"\n",
        "You are a document analyst.\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.\n",
        "You follow these instructions and use only these documents.\n",
        "\n",
        "Here is a set of Q+A pairs:\n",
        "\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\n",
        "When answering, clearly state the source and page number in parentheses after each key fact or figure.\n",
        "\n",
        "If information is drawn from the context, include a citation in this format: (source: {{source_file}}, page {{page_num}}).\n",
        "\n",
        "Answer with references:\n",
        "\n",
        "Add a reference list at the end, listing all the references used in format: source: {{source_file}}, page {{page_num}}. In the list, do not repeat the same reference more than one time.\n",
        "\"\"\"\n",
        "\n",
        "individual_prompt = ChatPromptTemplate.from_template(individual_template1)\n",
        "\n",
        "individual_final = (\n",
        "    individual_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "individual_answer= individual_final.invoke({\"context\":context,\"question\":question})\n",
        "\n",
        "display(Markdown(individual_answer))"
      ],
      "metadata": {
        "id": "CJiXqURnuZOL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f7d83c2a-e8b5-487b-9f59-595056967c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used a total of 57 kilotons (or 57,000 tonnes) of Sustainable Aviation Fuel (SAF) (source: KLM Sustainability, page 42). This represents a significant increase from the previous year, where KLM consumed 49 kilotons of SAF in 2023 (source: KLM Sustainability, page 42).\n\n### References:\nsource: KLM Sustainability, page 42."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG-Fusion"
      ],
      "metadata": {
        "id": "-6N_9tAjWlK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It rephrases the question a defined number of times, keeping its meaning.\n",
        "\n",
        "Example: \"How much SAF did KLM use in 2024?\" → \"What is KLM SAF usage in 2024?\", ..."
      ],
      "metadata": {
        "id": "KnU0ydaFiGtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAGf_template = \"\"\"\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies.\n",
        "You follow these instructions only.\n",
        "You are a helpful assistant that generates multiple search queries based on a single input query.\n",
        "Generate multiple search queries related to: {question}\n",
        "Output (3 queries):\"\"\"\n",
        "\n",
        "RAGf_prompt = ChatPromptTemplate.from_template(RAGf_template)\n",
        "\n",
        "generate_queries = (\n",
        "    RAGf_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.strip().split(\"\\n\")[:3])\n",
        ")\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\"Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "\n",
        "    fused_scores = {}\n",
        "\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    return [doc for doc, _ in reranked_results]\n",
        "\n",
        "\n",
        "retrieval_RAGf = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "\n",
        "docs = retrieval_RAGf.invoke({question})\n",
        "\n",
        "formatted_docs = \"\\n\\n\".join(\n",
        "    f\"Source: {doc.metadata.get('source_file', 'Unknown Source')}\\n\"\n",
        "    f\"Page: {doc.metadata.get('page_num', 'Unknown Page')}\\n\"\n",
        "    f\"Content: {doc.page_content}\"\n",
        "    for doc in docs\n",
        ")\n",
        "\n",
        "\n",
        "RAGf_template = \"\"\"\n",
        "You are a document analyst.\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.\n",
        "You follow these instructions and use only these documents.\n",
        "\n",
        "Answer the following question using only the provided context.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "When quoting a fact, cite it like this:\n",
        "(source: {{source_file}}, page {{page_num}})\n",
        "\n",
        "At the very end add a reference list, one per line, e.g.:\n",
        "- source: {{source_file}}, page {{page_num}}. In the list, do not repeat the same reference more than one time.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "RAGf_prompt = ChatPromptTemplate.from_template(RAGf_template)\n",
        "\n",
        "prepare_input = RunnablePassthrough.assign(\n",
        "    context=itemgetter(\"context\"),\n",
        "    question=itemgetter(\"question\")\n",
        ")\n",
        "\n",
        "RAGf_final = (\n",
        "    prepare_input\n",
        "    | RAGf_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "RAGf_answer= RAGf_final.invoke({\"question\":question, \"context\": formatted_docs})\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(RAGf_answer))"
      ],
      "metadata": {
        "id": "CqXbMTpbWkHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "ed55553e-23df-4d34-ad00-39e19f801c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-19-1257401244.py:33: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used 57 Ktons of Sustainable Aviation Fuel (SAF), which represents a 17.1% increase from the 49 Ktons used in 2023 (source: KLM Sustainability, page 42).\n\nReference list:\n- source: KLM Sustainability, page 42."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HyDE"
      ],
      "metadata": {
        "id": "tP-zOG6sWoXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First generates an hypothetical answer, and then uses it instead of the question to retrieve the relevant information.\n",
        "\n",
        "The concept is that the question should be clarified and rewritten in terms used in the documents."
      ],
      "metadata": {
        "id": "fZQV3AO6kDFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyDE document genration (no use of retrieved docs, hypothetical answer)\n",
        "HyDE_template0 = \"\"\"Please answer the question\n",
        "Question: {question}\n",
        "Passage:\"\"\"\n",
        "HyDE_prompt0 = ChatPromptTemplate.from_template(HyDE_template0)\n",
        "\n",
        "\n",
        "HyDE_retrieval0 = (\n",
        "    HyDE_prompt0 | chat | StrOutputParser()\n",
        ")\n",
        "\n",
        "hypothetical_ans = HyDE_retrieval0.invoke({\"question\": question})\n",
        "display(Markdown(hypothetical_ans))\n",
        "\n",
        "# Use that hypothetical ans for retrieval\n",
        "HyDE_retrieval = retriever.invoke(hypothetical_ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "fhH1lRZMxeGZ",
        "outputId": "be364ccc-4854-4f4f-cc17-35b54dcceb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry, but I don't have access to specific data or events that occurred in 2024, including the amount of Sustainable Aviation Fuel (SAF) used by KLM. You may want to check KLM's official reports or news releases for the most accurate and up-to-date information."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG\n",
        "HyDE_template = \"\"\"\n",
        "You are a document analyst.\n",
        "You follow these instructions and use only the documents provided.\n",
        "\n",
        "Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- When answering, clearly state the source and page number in parentheses after each key fact or figure.\n",
        "- Use this exact format for citations: (source: {{source_file}}, page {{page_num}})\n",
        "- If multiple facts come from the same page, cite it each time in the text, but only list it ONCE in the reference list.\n",
        "- Do NOT repeat the same source and page combination more than once in the reference list.\n",
        "\n",
        "Answer with references:\n",
        "\n",
        "At the end, add a 'Reference List' with only UNIQUE entries in the format:\n",
        "- source: {{source_file}}, page {{page_num}}\n",
        "\"\"\"\n",
        "\n",
        "HyDE_prompt = ChatPromptTemplate.from_template(HyDE_template)\n",
        "\n",
        "HyDE_final = (\n",
        "    HyDE_prompt\n",
        "    | chat\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "HyDE_answer= HyDE_final.invoke({\"context\":HyDE_retrieval,\"question\":question})\n",
        "\n",
        "display(Markdown(HyDE_answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "MTm9qCpD-w8F",
        "outputId": "79c60fa8-7bb8-4230-eb7b-edfc7f7d8c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used a total of 57 kilotons of Sustainable Aviation Fuel (SAF), which represents a 17.1% increase from the previous year (source: KLM Sustainability, page 42). Additionally, the SAF savings achieved by KLM amounted to 165 kilotons (source: KLM Sustainability, page 42).\n\nReference List:\n- source: KLM Sustainability, page 42"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL Q&A"
      ],
      "metadata": {
        "id": "xbLGXUZtC8F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"question: {question}\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"Naive answer:\")\n",
        "display(Markdown(chat.invoke(naive_template).content))\n",
        "\n",
        "print(\"\\n\\nRecursive answer:\")\n",
        "display(Markdown(recursive_answer))\n",
        "\n",
        "print(\"\\n\\nIndividual answer:\")\n",
        "display(Markdown(individual_answer))\n",
        "\n",
        "print(\"\\n\\nRAG-Fusion answer:\")\n",
        "display(Markdown(RAGf_answer))\n",
        "\n",
        "print(\"\\n\\nHyDE answer:\")\n",
        "display(Markdown(HyDE_answer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8WNbTXNEC_S5",
        "outputId": "ac7c49eb-20f5-4ce4-e2fc-c68cb04e150f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: How much SAF did KLM use in 2024?\n",
            "\n",
            "\n",
            "Naive answer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used 57 kilotons of Sustainable Aviation Fuel (SAF). \n\nReferences:\n- KLM Sustainability, page 42."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Recursive answer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM's total fuel consumption was 3,273 kilotons of conventional aviation fuel, and it used 57 kilotons of Sustainable Aviation Fuel (SAF) (source: KLM Sustainability, page 42). To calculate the percentage of KLM's total fuel consumption that was made up of SAF, we can use the following formula:\n\n\\[\n\\text{Percentage of SAF} = \\left( \\frac{\\text{SAF used}}{\\text{Total fuel consumption}} \\right) \\times 100\n\\]\n\nSubstituting the values:\n\n\\[\n\\text{Percentage of SAF} = \\left( \\frac{57 \\text{ kilotons}}{3,273 \\text{ kilotons}} \\right) \\times 100 \\approx 1.74\\%\n\\]\n\nThus, approximately **1.74%** of KLM's total fuel consumption in 2024 was made up of Sustainable Aviation Fuel (SAF).\n\n### References\nsource: KLM Sustainability, page 42"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Individual answer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used a total of 57 kilotons (or 57,000 tonnes) of Sustainable Aviation Fuel (SAF) (source: KLM Sustainability, page 42). This represents a significant increase from the previous year, where KLM consumed 49 kilotons of SAF in 2023 (source: KLM Sustainability, page 42).\n\n### References:\nsource: KLM Sustainability, page 42."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "RAG-Fusion answer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used 57 Ktons of Sustainable Aviation Fuel (SAF), which represents a 17.1% increase from the 49 Ktons used in 2023 (source: KLM Sustainability, page 42).\n\nReference list:\n- source: KLM Sustainability, page 42."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "HyDE answer:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In 2024, KLM used a total of 57 kilotons of Sustainable Aviation Fuel (SAF), which represents a 17.1% increase from the previous year (source: KLM Sustainability, page 42). Additionally, the SAF savings achieved by KLM amounted to 165 kilotons (source: KLM Sustainability, page 42).\n\nReference List:\n- source: KLM Sustainability, page 42"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation: Ragas"
      ],
      "metadata": {
        "id": "th920GCP6O57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness"
      ],
      "metadata": {
        "id": "Dfo4dwovWqUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions and Ground Truths"
      ],
      "metadata": {
        "id": "0nGjKnJKnZpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import answer_correctness, faithfulness, NonLLMContextRecall\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "# Upload and load JSON file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = next(iter(uploaded))\n",
        "with open(file_name, \"r\") as f:\n",
        "    ground_truth_data = json.load(f)\n",
        "\n",
        "# Create GROUND_TRUTH_MAP from file\n",
        "GROUND_TRUTH_MAP = {\n",
        "    item[\"question\"]: {\n",
        "        \"ground_truth\": item[\"ground_truth_answer\"]\n",
        "    }\n",
        "    for item in ground_truth_data\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "js0eHeifLtE6",
        "outputId": "607e1682-77bb-40a2-c3fc-f1c4e272d4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-59614154-5e75-43b6-a837-ded96a6f33f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-59614154-5e75-43b6-a837-ded96a6f33f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Evaluation.json to Evaluation.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "7Rhoj8MAnfjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NAIVE\n",
        "\n",
        "def run_naive(question: str) -> dict:\n",
        "    docs = retriever.invoke(question)\n",
        "\n",
        "    doc_references = \"\\n\\n\".join(\n",
        "        f\"Title: {doc.metadata.get('source_file', 'Unknown Source')}\\nPage: {doc.metadata.get('page_num', 'Unknown Page')}\\nContent: {doc.page_content}\"\n",
        "        for doc in docs\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=\"\"\" You are a document analyst.\n",
        "The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.\n",
        "You follow these instructions and use only these documents.\n",
        "You reference the sources in a reference list at the end, using the original source file name under 'source_file' and the page number under 'page_num'. In the list, don't repeat the same reference multiple times.\n",
        "You can recognize when no document is a good match and return \"I don't know\". \"\"\"),\n",
        "        HumanMessage(content=\"Here are the documents: \" + doc_references),\n",
        "        HumanMessage(content=\"Here are the instructions: \" + question)\n",
        "    ]\n",
        "\n",
        "    answer = chat(messages).content\n",
        "\n",
        "    ground_truth_info = GROUND_TRUTH_MAP.get(question, {})\n",
        "    ground_truth_answer = ground_truth_info.get(\"ground_truth\", [])\n",
        "\n",
        "    return {\n",
        "        \"model_id\": \"naive\",\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"contexts\": [doc.page_content for doc in docs],\n",
        "        \"ground_truth\": ground_truth_answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "uWfDGkiCBnLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive\n",
        "def run_recursive(question: str) -> dict:\n",
        "    # Subquestions\n",
        "    subquestions = generate_queries_decomposition.invoke({\"question\": question})\n",
        "\n",
        "    q_a_pairs = \"\"\n",
        "    all_docs = []\n",
        "\n",
        "    for subq in subquestions:\n",
        "        # Retrieve docs for each subquestion\n",
        "        docs = retriever.invoke(subq)\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "        context_text = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "        # Format prompt for subquestion\n",
        "        formatted_prompt = recursive_prompt.format_prompt(\n",
        "            question=subq,\n",
        "            context=context_text,\n",
        "            q_a_pairs=q_a_pairs or \"None yet\"\n",
        "        )\n",
        "\n",
        "        # Invoke chat with formatted prompt\n",
        "        sub_answer = chat.invoke(formatted_prompt).content\n",
        "\n",
        "        # Append Q&A pair\n",
        "        q_a_pairs += \"\\n---\\n\" + format_qa_pair(subq, sub_answer)\n",
        "\n",
        "    # Aggregate all docs content for final prompt context\n",
        "    final_context = \"\\n\\n\".join(doc.page_content for doc in all_docs)\n",
        "\n",
        "    # Format final prompt for the original question\n",
        "    final_prompt = recursive_prompt.format_prompt(\n",
        "        question=question,\n",
        "        context=final_context,\n",
        "        q_a_pairs=q_a_pairs or \"None\"\n",
        "    )\n",
        "\n",
        "    # Final answer\n",
        "    final_answer = chat.invoke(final_prompt).content\n",
        "\n",
        "    # Deduplicate contexts\n",
        "    unique_contexts = list({doc.page_content for doc in all_docs})\n",
        "\n",
        "    # Use GROUND_TRUTH_MAP to get the ground truth answer\n",
        "    ground_truth_info = GROUND_TRUTH_MAP.get(question, {})\n",
        "    ground_truth_answer = ground_truth_info.get(\"ground_truth\", [])\n",
        "\n",
        "    return {\n",
        "        \"model_id\": \"recursive\",\n",
        "        \"question\": question,\n",
        "        \"answer\": final_answer,\n",
        "        \"contexts\": unique_contexts,\n",
        "        \"ground_truth\": ground_truth_answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "n2bx89OzB4LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual\n",
        "def run_individual(question: str) -> dict:\n",
        "    sub_questions = generate_queries_decomposition.invoke({\"question\": question})\n",
        "\n",
        "    subanswers = []\n",
        "    all_docs = []\n",
        "\n",
        "    for subq in sub_questions:\n",
        "        docs = retriever.invoke(subq)\n",
        "        all_docs.extend(docs)\n",
        "\n",
        "        # Context string\n",
        "        context = \"\\n\\n\".join(\n",
        "            f\"{doc.page_content}\\n(Source: {doc.metadata.get('source_file', 'Unknown')}, Page: {doc.metadata.get('page_num', 'N/A')})\"\n",
        "            for doc in docs\n",
        "        )\n",
        "\n",
        "        # Format the individual_template0 prompt with context and question\n",
        "        formatted_prompt = individual_template0.format_prompt(\n",
        "            context=context,\n",
        "            question=subq\n",
        "        )\n",
        "\n",
        "        # Invoke chat and get answer content\n",
        "        answer = chat.invoke(formatted_prompt).content\n",
        "\n",
        "        subanswers.append((subq, answer))\n",
        "\n",
        "    # Format Q+A pairs text for final synthesis prompt\n",
        "    q_a_pairs_text = \"\\n\\n\".join(\n",
        "        f\"Question: {q}\\nAnswer: {a}\" for q, a in subanswers\n",
        "    )\n",
        "\n",
        "    # Format final prompt to synthesize answers\n",
        "    final_prompt = individual_prompt.format_prompt(\n",
        "        context=q_a_pairs_text,\n",
        "        question=question\n",
        "    )\n",
        "\n",
        "\n",
        "    final_answer = chat.invoke(final_prompt).content\n",
        "\n",
        "    # Deduplicate contexts\n",
        "    unique_contexts = list({doc.page_content for doc in all_docs})\n",
        "\n",
        "    # Use GROUND_TRUTH_MAP to get the ground truth answer\n",
        "    ground_truth_info = GROUND_TRUTH_MAP.get(question, {})\n",
        "    ground_truth_answer = ground_truth_info.get(\"ground_truth\", [])\n",
        "\n",
        "    return {\n",
        "        \"model_id\": \"individual\",\n",
        "        \"question\": question,\n",
        "        \"answer\": final_answer,\n",
        "        \"contexts\": unique_contexts,\n",
        "        \"ground_truth\": ground_truth_answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WeSn-56OCECh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG-Fusion\n",
        "def run_ragf(question: str) -> dict:\n",
        "    docs = retrieval_RAGf.invoke({\"question\": question})\n",
        "\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    answer = RAGf_final.invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": context\n",
        "    })\n",
        "\n",
        "    # Use GROUND_TRUTH_MAP to get the ground truth answer\n",
        "    ground_truth_info = GROUND_TRUTH_MAP.get(question, {})\n",
        "    ground_truth_answer = ground_truth_info.get(\"ground_truth\", [])\n",
        "\n",
        "    return {\n",
        "        \"model_id\": \"rag_fusion\",\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"contexts\": [doc.page_content for doc in docs],\n",
        "        \"ground_truth\": ground_truth_answer\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# HyDE\n",
        "def run_hyde(question: str) -> dict:\n",
        "    hypothetical_answer = HyDE_retrieval0.invoke({\"question\": question})\n",
        "    docs = retriever.invoke(hypothetical_answer)\n",
        "\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    answer = HyDE_final.invoke({\n",
        "        \"context\": context,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    # Use GROUND_TRUTH_MAP to get the ground truth answer\n",
        "    ground_truth_info = GROUND_TRUTH_MAP.get(question, {})\n",
        "    ground_truth_answer = ground_truth_info.get(\"ground_truth\", [])\n",
        "\n",
        "    return {\n",
        "        \"model_id\": \"hyde\",\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"contexts\": [doc.page_content for doc in docs],\n",
        "        \"ground_truth\": ground_truth_answer\n",
        "    }\n"
      ],
      "metadata": {
        "id": "w_ij4rvHvaDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "vIil1rJxnjcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import AnswerCorrectness, faithfulness, LLMContextRecall\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from datasets import Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Environment\n",
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
        "os.environ[\"OPENAI_API_BASE\"] = API_ENDPOINT\n",
        "\n",
        "evaluator_llm  = LangchainLLMWrapper(chat)\n",
        "context_recall = LLMContextRecall(llm=evaluator_llm)\n",
        "\n",
        "answer_correctness = AnswerCorrectness(weights=[0.75, 0.25])\n",
        "\n",
        "all_results = []\n",
        "model_times = defaultdict(float)\n",
        "\n",
        "questions_to_run = list(GROUND_TRUTH_MAP.keys())\n",
        "\n",
        "for question in questions_to_run:\n",
        "    for run_fn in [run_naive, run_recursive, run_individual, run_ragf, run_hyde]:\n",
        "        model_id = run_fn.__name__.replace(\"run_\", \"\")\n",
        "        start = time.time()\n",
        "        result = run_fn(question)\n",
        "        end = time.time()\n",
        "        model_times[model_id] += end - start\n",
        "        all_results.append(result)\n",
        "\n",
        "# Print total time per model\n",
        "print(\"Total time per model:\")\n",
        "for model_id, total_time in model_times.items():\n",
        "    print(f\"  {model_id}: {total_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "dataset = Dataset.from_list(all_results)\n",
        "\n",
        "# Evaluate\n",
        "ragas_scores = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[answer_correctness, faithfulness, context_recall]\n",
        ")\n",
        "\n",
        "# Convert scores to pandas\n",
        "df_scores = ragas_scores.to_pandas()\n",
        "\n",
        "# Convert original results to pandas\n",
        "df_results = pd.DataFrame(all_results)\n",
        "\n",
        "# Merge on question + answer\n",
        "df = pd.concat([df_results[['model_id', 'question']], df_scores], axis=1)\n",
        "\n",
        "# Group by model_id\n",
        "overall_scores = (\n",
        "    df.groupby(\"model_id\")[[\"answer_correctness\", \"context_recall\", \"faithfulness\"]]\n",
        "      .mean()\n",
        "      .round(3)\n",
        "      .sort_values(\"answer_correctness\", ascending=False)\n",
        ")\n",
        "\n",
        "print(overall_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274,
          "referenced_widgets": [
            "69c97ae6b7524098b68349ae6aa34279",
            "1d1c18c7214b40ea84b995da5acf47da",
            "f53b2c1c1c5e45cbad184dfafd5930a6",
            "e98ffca62c59413ea975bbaf92dbc699",
            "fa6179e95784406b878302ed2533bb6e",
            "ceede3c16a9644a58978c5b560e57bda",
            "39b3e77d056349a49c62cdf1008c110e",
            "00630523779042f091c75a6919a6b1cd",
            "ae5b4328366842029749c2316e3e2e2e",
            "d60dae8c77ec4bae85a40945c409a783",
            "e48ba043a16142b5bfe18e0b7f42f5b7"
          ]
        },
        "id": "8kN_g8d59sCk",
        "outputId": "6c586df5-b58a-40e4-af19-d0974202ebf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time per model:\n",
            "  naive: 45.04 seconds\n",
            "  recursive: 466.59 seconds\n",
            "  individual: 326.20 seconds\n",
            "  ragf: 76.67 seconds\n",
            "  hyde: 101.01 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/300 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69c97ae6b7524098b68349ae6aa34279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            answer_correctness  context_recall  faithfulness\n",
            "model_id                                                    \n",
            "individual               0.620           0.702         0.833\n",
            "rag_fusion               0.586           0.636         0.751\n",
            "hyde                     0.563           0.474         0.922\n",
            "naive                    0.540           0.512         0.617\n",
            "recursive                0.481           0.711         0.859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHECK! should save a nice structured json. instead of previous so you can save the different runs (5?)\n",
        "\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def remove_contexts(d):\n",
        "    if isinstance(d, dict):\n",
        "        return {k: remove_contexts(v) for k, v in d.items() if k not in ['context', 'contexts']}\n",
        "    elif isinstance(d, list):\n",
        "        return [remove_contexts(i) for i in d]\n",
        "    else:\n",
        "        return d\n",
        "\n",
        "structured_results = defaultdict(dict)\n",
        "\n",
        "for res in all_results:\n",
        "    question = res['question']\n",
        "    model_id = res['model_id']\n",
        "    answer_data = {k: v for k, v in res.items() if k not in ['question', 'model_id']}\n",
        "    answer_data = remove_contexts(answer_data)\n",
        "    structured_results[question][model_id] = answer_data\n",
        "\n",
        "structured_results = dict(structured_results)\n",
        "\n",
        "with open(\"FIVE_results_by_question.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(structured_results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Results saved to FIVE_results_by_question.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV_rQgTq5QEg",
        "outputId": "8fdbe7be-d950-4367-f703-f51b20ff5916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to FIVE_results_by_question.json\n"
          ]
        }
      ]
    }
  ]
}