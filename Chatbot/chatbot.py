# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JPyMQ5VyRtzM4ITN2yLcJYcNxuIZqgv8
"""

!pip install  gradio langchain langchain-openai openai faiss-cpu langchain-community

import gradio as gr
import zipfile
import os
import json
from operator import itemgetter
from json import dumps, loads

from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage,AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    AIMessagePromptTemplate,
)


# API
API_KEY = "AAA"
API_ENDPOINT = "https://ai-research-proxy.azurewebsites.net"

# Global retriever variable
retriever = None

def load_faiss(zip_file):
    global retriever

    # Use the Gradio-provided file path directly
    with zipfile.ZipFile(zip_file.name, "r") as zip_ref:
        zip_ref.extractall("faiss_index")

    # Initialize embedding model
    embedding = OpenAIEmbeddings(
        openai_api_key=API_KEY,
        openai_api_base=API_ENDPOINT,
        model="text-embedding-ada-002"
    )

    # Load vectorstore and retriever
    vectorstore = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 10}
    )

    return "FAISS index successfully loaded."

chat = ChatOpenAI(
    openai_api_base=API_ENDPOINT,
    openai_api_key=API_KEY,
    model="gpt-4o-mini",
    temperature=0.1,
)

# RAG-Fusion

# Rephrase queries prompt
RAGf_query_template = """
The user will submit some documents containing the 2023-2024 reports of some aviation companies.
You follow these instructions only.
You are a helpful assistant that generates multiple search queries based on a single input query.
Generate multiple search queries related to: {question}
Output (3 queries):
"""
query_prompt = ChatPromptTemplate.from_template(RAGf_query_template)

generate_queries = (
    query_prompt
    | chat
    | StrOutputParser()
    | (lambda x: x.strip().split("\n")[:3])
)

# Reciprocal Rank Fusion
def reciprocal_rank_fusion(results: list[list], k=60):
    fused_scores = {}
    for docs in results:
        for rank, doc in enumerate(docs):
            doc_str = dumps(doc)
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            fused_scores[doc_str] += 1 / (rank + k)

    reranked_results = [
        (loads(doc), score) for doc, score in
        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    ]

    return [doc for doc, _ in reranked_results]

# Few-shot examples
few_shot_examples = [
    {
        "question": "By how much did Qantas's SAF usage increase from FY23 to FY24, both in litres and in percentage terms?",
        "answer": "Qantas's SAF usage increased from 8,508,616 litres in FY23 to 9,991,377 litres in FY24 — an increase of 1,482,761 litres or approximately 17.4%."
    },
    {
        "question": "What are the key policy and strategic measures KLM is implementing to support the use of Sustainable Aviation Fuel (SAF) by 2025 and beyond?",
        "answer": "KLM is complying with an EU mandate to blend 2% SAF by the end of 2024, offers voluntary SAF contributions to passengers, co-leads Project SkyPower to scale e-SAF production, and will apply a mandatory SAF surcharge starting in 2025."
    },
    {
        "question": "What was the average percentage increase in aviation fuel consumption from 2023 to 2024 across Qantas and KLM?",
        "answer": "Qantas increased aviation fuel consumption from 3,847,941,000 L in FY23 to 4,540,502,000 L in FY24 — an 18% increase. KLM increased from 9,972 kilotonnes in 2023 to 10,360 kilotonnes in 2024 — a 3.9% increase. The average increase across both airlines is approximately 10.95%."
    },
    {
        "question": "What was the amount of CO2 emissions produced by KLM in 2024?",
        "answer": "8,900 kilotons of CO2."
    },
    {
        "question": "How much fuel did Qantas save by shutting down two engines during parking in 2023?",
        "answer": "Up to 5,500 tonnes."
    },
    {
        "question": "How does United ensure that its environmental policy is implemented and adapted over time?",
        "answer": "United ensures that its environmental policy is implemented and adapted over time through a structured environmental management system following Plan-Do-Check-Act, supported by around 40 environmental coordinators, regular audits, and frequent updates aligned with new research and stakeholder feedback."
    },
]

# Answer generation prompt with few-shot integrated
def make_fewshot_prompt():
    # 4a) system instructions
    system_tmpl = SystemMessagePromptTemplate.from_template(
        "You are a document analyst.\n"
        "1) First you will see several Q&A examples illustrating the format.\n"
        "2) Then you will receive document excerpts under “Retrieved documents:”.\n"
        "3) Finally you will get a new question.\n"
        "Use *only* the excerpts to answer; if it was already answered in an example, answer from the excerpts.\n"
        "If you can’t find the answer in the excerpts, reply exactly “I don't know.”\n"
        "Cite each fact inline like this: (source: <source_file>, page <page_num>).\n"
        "At the end, add a de-duplicated reference list, one line per reference:\n"
        "- <source_file>, page <page_num>"
    )

    # 4b) few-shot example templates
    example_tmpls = []
    for ex in few_shot_examples:
        example_tmpls.append(
            HumanMessagePromptTemplate.from_template("Q: " + ex["question"])
        )
        example_tmpls.append(
            AIMessagePromptTemplate.from_template("A: " + ex["answer"])
        )

    # 4c) dynamic context & question slots
    retrieval_tmpl = HumanMessagePromptTemplate.from_template(
        "Retrieved documents:\n\n{context}"
    )
    question_tmpl = HumanMessagePromptTemplate.from_template("Q: {question}")
    answer_tmpl   = HumanMessagePromptTemplate.from_template("Answer:")

    # 4d) assemble the chat prompt
    return ChatPromptTemplate.from_messages([
        system_tmpl,
        *example_tmpls,
        retrieval_tmpl,
        question_tmpl,
        answer_tmpl,
    ])

# 5) Build your final chain using a placeholder prompt
RAGf_prompt = make_fewshot_prompt()
prepare_input = RunnablePassthrough.assign(
    context=itemgetter("context"),
    question=itemgetter("question")
)

RAGf_final_chain = (
    prepare_input
    | RAGf_prompt
    | chat
    | StrOutputParser()
)

# Ask question
def ask_question(question):
    if retriever is None:
        return "Please upload and load a FAISS index first."

    # Subquery generation
    raw_qs = generate_queries.invoke({"question": question})
    queries = [q.lstrip("0123456789. -").strip() for q in raw_qs][:3]

    # Retrieve & fuse
    all_results = []
    for q in queries:
        try:
            docs = retriever.get_relevant_documents(q)
        except:
            docs = retriever.invoke({"query": q})
        all_results.append([{"page_content": d.page_content, "metadata": d.metadata} for d in docs])

    fused = reciprocal_rank_fusion(all_results)
    if not fused:
        # Fallback zero-shot
        return chat([
            SystemMessage(content="You are a document analyst. Answer from your knowledge when no context is found."),
            HumanMessage(content=f"Question: {question}")
        ]).content

    # Format fused into a single context string
    formatted_entries = []
    for doc in fused:
        src  = doc["metadata"].get("source_file", "Unknown Source")
        pg   = doc["metadata"].get("page_num",    "Unknown Page")
        txt  = doc["page_content"]
        formatted_entries.append(f"Source: {src}\nPage: {pg}\nContent:\n{txt}")
    formatted_docs = "\n\n---\n\n".join(formatted_entries)

    # Rebuild prompt _with_ real context & question
    RAGf_prompt = make_fewshot_prompt()
    prompt_value = RAGf_prompt.format_prompt(
        context=formatted_docs,
        question=question
    )

    # Invoke the model
    return chat(prompt_value.to_messages()).content

# Gradio interfaces
upload_interface = gr.Interface(
    fn=load_faiss,
    inputs=gr.File(label="Upload FAISS .zip", file_types=[".zip"]),
    outputs="text",
    title="Upload Your FAISS Index"
)

chat_interface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(label="Ask a question"),
    outputs="markdown",
    title="Airlines' Sustainability Reports Chatbot"
)

gr.TabbedInterface(
    [upload_interface, chat_interface],
    ["Upload FAISS", "Ask a Question"]
).launch(share=True, debug=True)

