# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dMFWwXl1uD2uagsYbJlgDN3hFMiaev5T
"""

!pip install  -q gradio langchain langchain-openai openai faiss-cpu langchain-community

import gradio as gr
import zipfile
import os
import json
from operator import itemgetter
from json import dumps, loads

from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# API
API_KEY = "AAA"
API_ENDPOINT = "https://ai-research-proxy.azurewebsites.net"

# Global retriever variable
retriever = None

def load_faiss(zip_file):
    global retriever

    # Use the Gradio-provided file path directly
    with zipfile.ZipFile(zip_file.name, "r") as zip_ref:
        zip_ref.extractall("faiss_index")

    # Initialize embedding model
    embedding = OpenAIEmbeddings(
        openai_api_key=API_KEY,
        openai_api_base=API_ENDPOINT,
        model="text-embedding-ada-002"
    )

    # Load vectorstore and retriever
    vectorstore = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 10}
    )

    return "FAISS index successfully loaded."

chat = ChatOpenAI(
    openai_api_base= API_ENDPOINT,
    openai_api_key= API_KEY,
    model = "gpt-4o-mini",
    temperature=0.1,
)



# RAG-Fusion
from langchain.schema import HumanMessage, AIMessage, SystemMessage

few_shot_examples = [
    {
        "question": "By how much did Qantas's SAF usage increase from FY23 to FY24, both in litres and in percentage terms?",
        "answer": "Qantas's SAF usage increased from 8,508,616 litres in FY23 to 9,991,377 litres in FY24 — an increase of 1,482,761 litres or approximately 17.4%."
    },
    {
        "question": "What are the key policy and strategic measures KLM is implementing to support the use of Sustainable Aviation Fuel (SAF) by 2025 and beyond?",
        "answer": "KLM is complying with an EU mandate to blend 2% SAF by the end of 2024, offers voluntary SAF contributions to passengers, co-leads Project SkyPower to scale e-SAF production, and will apply a mandatory SAF surcharge starting in 2025."
    },
    {
        "question": "What was the average percentage increase in aviation fuel consumption from 2023 to 2024 across Qantas and KLM?",
        "answer": "Qantas increased aviation fuel consumption from 3,847,941,000 L in FY23 to 4,540,502,000 L in FY24 — an 18% increase. KLM increased from 9,972 kilotonnes in 2023 to 10,360 kilotonnes in 2024 — a 3.9% increase. The average increase across both airlines is approximately 10.95%."
    },
    {
        "question": "What was the amount of CO2 emissions produced my KLM in 2024 ",
        "answer": "8,900 kilotons of CO2."
    },
    {
        "question": "How much fuel did Qantas save by shutting down two engines during parking in 2023? ",
        "answer": "Up to 5500 tonnes"

    },
    {
        "question": "How does United ensure that its environmental policy is implemented and adapted over time? ",
        "answer": "United ensures that its environmental policy is implemented and adapted over time through a structured environmental management system that follows the principle of Plan-Do-Check-Act. This system is designed for continuous improvement and is supported by around 40 environmental coordinators across different departments. The company management monitors the effectiveness of the management system and provides necessary resources. Additionally, the environmental guidelines are updated regularly in line with developments in research, technology, and social debate. Regular audits and sustainability updates for management teams also contribute to the ongoing adaptation and effectiveness of the environmental policy."

    },
]

# Convert few-shot examples to messages
few_shot_messages = []
for ex in few_shot_examples:
    few_shot_messages.append(
        HumanMessage(content=f"Question: {ex['question']}\nAnswer:")
    )
    few_shot_messages.append(
        AIMessage(content=ex["answer"])
    )

# Rephrase queries prompt
RAGf_query_template = """
The user will submit some documents containing the 2023-2024 reports of some aviation companies.
You follow these instructions only.
You are a helpful assistant that generates multiple search queries based on a single input query.
Generate multiple search queries related to: {question}
Output (3 queries):
"""
query_prompt = ChatPromptTemplate.from_template(RAGf_query_template)

generate_queries = (
    query_prompt
    | chat
    | StrOutputParser()
    | (lambda x: x.strip().split("\n")[:3])
)

# Reciprocal Rank Fusion
def reciprocal_rank_fusion(results: list[list], k=60):
    fused_scores = {}
    for docs in results:
        for rank, doc in enumerate(docs):
            doc_str = dumps(doc)
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            previous_score = fused_scores[doc_str]
            fused_scores[doc_str] += 1 / (rank + k)

    reranked_results = [
        (loads(doc), score) for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    ]

    return [doc for doc, _ in reranked_results]


# Answer generation prompt
RAGf_answer_template = """
You are a document analyst.
The user will submit some documents containing the 2023-2024 reports of some aviation companies, the name of the company is under 'source file'.
You follow these instructions and use only these documents.

Answer the following question using only the provided context.

{context}

Question: {question}

When quoting a fact, cite it like this:
(source: {{source_file}}, page {{page_num}})

At the very end add a reference list, one per line, e.g.:
- source: {{source_file}}, page {{page_num}}. In the list, do not repeat the same reference more than one time.

Answer:
"""
RAGf_prompt = ChatPromptTemplate.from_template(RAGf_answer_template)

prepare_input = RunnablePassthrough.assign(
    context=itemgetter("context"),
    question=itemgetter("question")
)

RAGf_final_chain = (
    prepare_input
    | RAGf_prompt
    | chat
    | StrOutputParser()
)



# Ask question



def ask_question(question):
    if retriever is None:
        return "Please upload and load a FAISS index first."

    try:
        # Generate sub‐queries
        raw_queries = generate_queries.invoke({"question": question})
        def clean_query_line(line):
            return line.lstrip("0123456789. -").strip()
        queries = [clean_query_line(q) for q in raw_queries][:3]

        # Retrieve documents per query
        all_results = []
        for q in queries:
            try:
                docs_for_q = retriever.get_relevant_documents(q)
            except Exception:
                try:
                    docs_for_q = retriever.invoke({"query": q})
                except Exception as e:
                    print(f"Retrieval failed for '{q}': {e}")
                    docs_for_q = []
            dict_docs = [
                {"page_content": doc.page_content, "metadata": doc.metadata}
                for doc in docs_for_q
            ]
            all_results.append(dict_docs)

        # Fuse results
        fused = reciprocal_rank_fusion(all_results)
        top_docs = fused
        if not top_docs:
            return "I don't know."

        # Format retrieved context
        formatted_entries = []
        for doc in top_docs:
            source = doc["metadata"].get("source_file", "Unknown Source")
            page   = doc["metadata"].get("page_num",    "Unknown Page")
            content = doc["page_content"]
            entry = (
                f"Source: {source}\n"
                f"Page:   {page}\n"
                f"Content:\n{content}"
            )
            formatted_entries.append(entry)

        formatted_docs = "\n\n---\n\n".join(formatted_entries)

        # Build the chat messages with few‐shots + citation enforcement
        messages = [
            SystemMessage(content=(
                "You are a document analyst. Use *only* the provided context."
            )),
            *few_shot_messages,
            SystemMessage(content=(
                "IMPORTANT—now follow these EXACT rules:\n"
                "• Cite inline: (source: reportX.pdf, page 12)\n"
                "• Then at the very end, add a reference list, one per line:\n"
                "  - reportX.pdf, page 12\n"
                "• Do not repeat references."
            )),
            HumanMessage(content=(
                f"Context:\n{formatted_docs}\n\n"
                f"Question: {question}\nAnswer:"
            ))
        ]

        # Invoke the chat model
        resp = chat(messages)
        return resp.content

    except Exception as e:
        print("Error in ask_question:", e)
        return "Error processing your question."



# Gradio interfaces
upload_interface = gr.Interface(
    fn=load_faiss,
    inputs=gr.File(label="Upload FAISS .zip", file_types=[".zip"]),
    outputs="text",
    title="Upload Your FAISS Index"
)

chat_interface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(label="Ask a question"),
    outputs="markdown",
    title="Airlines' Sustainability Reports Chatbot"
)

gr.TabbedInterface(
    [upload_interface, chat_interface],
    ["Upload FAISS", "Ask a Question"]
).launch(share=True)

